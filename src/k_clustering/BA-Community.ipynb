{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polished-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn import tree, linear_model\n",
    "\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import umap\n",
    "\n",
    "from torch_geometric.nn import GNNExplainer\n",
    "\n",
    "from utilities import *\n",
    "from heuristics import *\n",
    "from activation_classifier import *\n",
    "import random\n",
    "from models import *\n",
    "\n",
    "set_rc_params()\n",
    "\n",
    "# ensure reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "positive-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general parameters\n",
    "dataset_name = \"BA_Community\"\n",
    "\n",
    "model_type = BA_Community_GCN\n",
    "load_pretrained = False\n",
    "\n",
    "# hyperparameters\n",
    "k = 20\n",
    "\n",
    "# other parameters\n",
    "train_test_split = 0.8\n",
    "num_hidden_units = 30\n",
    "num_classes = 8\n",
    "\n",
    "epochs = 7000\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reverse-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Node Classification\n",
      "Number of features:  1400\n",
      "Number of labels:  1400\n",
      "Number of classes:  1400\n",
      "Number of edges:  1400\n",
      "Epoch: 4095, Loss: 0.01907, Train Acc: 0.99910, Test Acc: 0.68836\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     16\u001B[0m     model\u001B[38;5;241m.\u001B[39mapply(weights_init)\n\u001B[1;32m---> 17\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbase\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\CAMBRIDGE\\GCExplainer\\src\\k_clustering\\models.py:425\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, data, epochs, lr, path)\u001B[0m\n\u001B[0;32m    423\u001B[0m \u001B[38;5;66;03m# calculate loss\u001B[39;00m\n\u001B[0;32m    424\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnll_loss(out[train_mask], y[train_mask])\n\u001B[1;32m--> 425\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    426\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CGN\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CGN\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "paths = prepare_output_paths(dataset_name, k)\n",
    "\n",
    "G, labels = load_syn_data(dataset_name)\n",
    "data = prepare_syn_data(G, labels, train_test_split, if_adj=True)\n",
    "model = model_type(data[\"x\"].shape[1], num_hidden_units, num_classes)\n",
    "\n",
    "if load_pretrained:\n",
    "    print(\"Loading pretrained model...\")\n",
    "    model.load_state_dict(torch.load(os.path.join(paths['base'], \"model.pkl\")))\n",
    "    model.eval()\n",
    "    \n",
    "    with open(os.path.join(paths['base'], \"activations.txt\"), 'rb') as file:\n",
    "        activation_list = pickle.loads(file.read())\n",
    "        \n",
    "else:\n",
    "    model.apply(weights_init)\n",
    "    train(model, data, 7000, lr, paths['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"x\"]\n",
    "edges = data[\"edges\"]\n",
    "y = data[\"y\"]\n",
    "train_mask = data[\"train_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "print(test(model, x, y, edges, test_mask))\n",
    "print(test(model, x, y, edges, train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    print(activation.shape)\n",
    "    \n",
    "print(paths['base'])\n",
    "print(paths['DBSCAN'])\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"tree_cycle_model.pkl\")\n",
    "\n",
    "with open(\"tree_cycle_activations.txt\", 'wb') as file:\n",
    "    pickle.dump(activation_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-globe",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE conversion\n",
    "tsne_models = []\n",
    "tsne_data = []\n",
    "\n",
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    tsne_model = TSNE(n_components=2)\n",
    "    d = tsne_model.fit_transform(activation)\n",
    "    plot_activation_space(d, labels, \"t-SNE reduced\", layer_num, paths['TSNE'], \"(coloured by labels)\")\n",
    "    \n",
    "    tsne_models.append(tsne_model)\n",
    "    tsne_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-consolidation",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA conversion\n",
    "pca_models = []\n",
    "pca_data = []\n",
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    pca_model = PCA(n_components=2)\n",
    "    d = pca_model.fit_transform(activation)\n",
    "    plot_activation_space(d, labels, \"PCA reduced\", layer_num, paths['PCA'], \"(coloured by labels)\")\n",
    "\n",
    "    pca_models.append(pca_model)\n",
    "    pca_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-national",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP conversion\n",
    "umap_models = []\n",
    "umap_data = []\n",
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    umap_model = umap.UMAP(n_components=2)\n",
    "    d = umap_model.fit_transform(activation)\n",
    "    plot_activation_space(d, labels, \"UMAP reduced\", layer_num, paths['UMAP'], \"(coloured by labels)\")\n",
    "\n",
    "    umap_models.append(umap_model)\n",
    "    umap_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-catch",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-brooks",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "\n",
    "##### RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_view = 5\n",
    "num_expansions = 2\n",
    "edges = data['edge_list'].numpy()\n",
    "\n",
    "raw_sample_graphs = []\n",
    "raw_kmeans_models = []\n",
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_model = kmeans_model.fit(activation)\n",
    "    pred_labels = kmeans_model.predict(activation)\n",
    "        \n",
    "    plot_clusters(tsne_data[layer_num], pred_labels, \"k-Means\", k, layer_num, paths['KMeans'], \"Raw\", \"_TSNE\", \"(t-SNE reduced)\")\n",
    "    plot_clusters(pca_data[layer_num], pred_labels, \"k-Means\", k, layer_num, paths['KMeans'], \"Raw\", \"_PCA\", \"(PCA reduced)\")\n",
    "    plot_clusters(umap_data[layer_num], pred_labels, \"k-Means\", k, layer_num, paths['KMeans'], \"Raw\", \"_UMAP\", \"(UMAP reduced)\")\n",
    "    sample_graphs, sample_feat = plot_samples(kmeans_model, activation, data[\"y\"], layer_num, k, \"Kmeans\", \"raw\", num_nodes_view, edges, num_expansions, paths['KMeans'])\n",
    "\n",
    "    raw_sample_graphs.append(sample_graphs)\n",
    "    raw_kmeans_models.append(kmeans_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-afghanistan",
   "metadata": {},
   "source": [
    "##### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sample_graphs = []\n",
    "tsne_sample_feat = []\n",
    "tsne_kmeans_models = []\n",
    "for layer_num, item in enumerate(tsne_data):\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_model = kmeans_model.fit(item)\n",
    "    pred_labels = kmeans_model.predict(item)\n",
    "\n",
    "    plot_clusters(item, pred_labels, \"k-Means\", k, layer_num, paths['KMeans'], \"t-SNE reduced\")\n",
    "    sample_graphs, sample_feat = plot_samples(kmeans_model, item, data[\"y\"], layer_num, k, \"k-Means\", \"t-SNE reduced\", num_nodes_view, edges, num_expansions, paths['KMeans'])\n",
    "\n",
    "    tsne_sample_graphs.append(sample_graphs)\n",
    "    tsne_sample_feat.append(sample_feat)\n",
    "    tsne_kmeans_models.append(kmeans_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-juvenile",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sample_graphs = []\n",
    "pca_sample_feat = []\n",
    "pca_kmeans_models = []\n",
    "\n",
    "for layer_num, item in enumerate(pca_data):\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_model = kmeans_model.fit(item)\n",
    "    pred_labels = kmeans_model.predict(item)\n",
    "\n",
    "    plot_clusters(item, pred_labels, \"k-Means\", k, layer_num, paths['KMeans'], \"PCA reduced\")\n",
    "    sample_graphs, sample_feat = plot_samples(kmeans_model, item, data[\"y\"], layer_num, k, \"k-Means\", \"PCA reduced\", num_nodes_view, edges, num_expansions, paths['KMeans'])\n",
    "\n",
    "    pca_sample_graphs.append(sample_graphs)\n",
    "    pca_sample_feat.append(sample_feat)\n",
    "    pca_kmeans_models.append(kmeans_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-terror",
   "metadata": {},
   "source": [
    "##### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_sample_graphs = []\n",
    "umap_sample_feat = []\n",
    "umap_kmeans_models = []\n",
    "for layer_num, item in enumerate(umap_data):\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_model = kmeans_model.fit(item)\n",
    "    pred_labels = kmeans_model.predict(item)\n",
    "\n",
    "    plot_clusters(item, pred_labels, \"k-Means\", k, layer_num, paths['KMeans'], \"UMAP reduced\")\n",
    "    sample_graphs, sample_feat = plot_samples(kmeans_model, item, data[\"y\"], layer_num, k, \"k-Means\", \"UMAP reduced\", num_nodes_view, edges, num_expansions, paths['KMeans'])\n",
    "\n",
    "    umap_sample_graphs.append(sample_graphs)\n",
    "    umap_sample_feat.append(sample_feat)\n",
    "    umap_kmeans_models.append(kmeans_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-moses",
   "metadata": {},
   "source": [
    "#### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ba_heuristics = BA_Shapes_Heuristics()\n",
    "\n",
    "# for layer_num, sample in enumerate(raw_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"k-Means\", \"raw\", paths['KMeans'])\n",
    "\n",
    "# for layer_num, sample in enumerate(tsne_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"k-Means\", \"t-SNE reduced\", paths['KMeans'])\n",
    "\n",
    "# for layer_num, sample in enumerate(pca_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"k-Means\", \"PCA reduced\", paths['KMeans'])\n",
    "\n",
    "# # for layer_num, sample in enumerate(umap_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"k-Means\", \"UMAP reduced\", paths['KMeans'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-start",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-conducting",
   "metadata": {},
   "source": [
    "##### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    plot_dendrogram(activation, \"raw\", layer_num, paths['Ward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_n_clusters = [3, 3, 12, 13]\n",
    "\n",
    "raw_sample_graphs = []\n",
    "raw_sample_feat = []\n",
    "raw_ward_models = []\n",
    "for layer_num, (key, n) in enumerate(zip(activation_list, raw_n_clusters)):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    hc = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='ward')\n",
    "    pred_labels = hc.fit_predict(activation)\n",
    "\n",
    "    d = (activation, pred_labels)\n",
    "    plot_clusters(tsne_data[layer_num], pred_labels, \"HC-Ward\", n, layer_num, paths['Ward'], \"raw\", \"_TSNE\", \"(t-SNE reduced)\")\n",
    "    plot_clusters(pca_data[layer_num], pred_labels, \"HC-Ward\", n, layer_num, paths['Ward'], \"raw\", \"_PCA\", \"(PCA reduced)\")\n",
    "    plot_clusters(pca_data[layer_num], pred_labels, \"HC-Ward\", n, layer_num, paths['Ward'], \"raw\", \"_UMAP\", \"(UMAP reduced)\")\n",
    "    sample_graphs, sample_feat = plot_samples(hc, d, data[\"y\"], layer_num, n, \"HC\", \"raw\", num_nodes_view, edges, num_expansions, paths['Ward'])\n",
    "\n",
    "    raw_sample_graphs.append(sample_graphs)\n",
    "    raw_sample_feat.append(sample_feat)\n",
    "    raw_ward_models.append(hc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-terrorism",
   "metadata": {},
   "source": [
    "##### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num, item in enumerate(tsne_data):\n",
    "    plot_dendrogram(item, \"t-SNE\", layer_num, paths['Ward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_n_clusters = [7, 12, 12, 13]\n",
    "\n",
    "tsne_sample_graphs = []\n",
    "tsne_sample_feat = []\n",
    "tsne_hc_models = []\n",
    "for layer_num, (item, n) in enumerate(zip(tsne_data, tsne_n_clusters)):\n",
    "    hc = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='ward')\n",
    "    pred_labels = hc.fit_predict(item)\n",
    "\n",
    "    d = (item, pred_labels)\n",
    "    plot_clusters(item, pred_labels, \"HC\", n, layer_num, paths['Ward'], \"t-SNE\")\n",
    "    sample_graphs, sample_feat = plot_samples(hc, d, data[\"y\"], layer_num, n, \"HC\", \"t-SNE reduced\", num_nodes_view, edges, num_expansions, paths['Ward'])\n",
    "\n",
    "    tsne_sample_graphs.append(sample_graphs)\n",
    "    tsne_sample_feat.append(sample_feat)\n",
    "    tsne_hc_models.append(hc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-indication",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num, item in enumerate(pca_data):\n",
    "    plot_dendrogram(item, \"PCA\", layer_num, paths['Ward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_n_clusters = [7, 12, 12, 13]\n",
    "\n",
    "pca_sample_graphs = []\n",
    "pca_sample_feat = []\n",
    "pca_hc_models = []\n",
    "for layer_num, (item, n) in enumerate(zip(pca_data, pca_n_clusters)):\n",
    "    hc = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='ward')\n",
    "    pred_labels = hc.fit_predict(item)\n",
    "\n",
    "    d = (item, pred_labels)\n",
    "    plot_clusters(item, pred_labels, \"HC\", n, layer_num, paths['HC'], \"PCA\")\n",
    "    sample_graphs, sample_feat = plot_samples(hc, d, data[\"y\"], layer_num, n, \"HC\", \"PCA reduced\", num_nodes_view, edges, num_expansions, paths['HC'])\n",
    "\n",
    "    pca_sample_graphs.append(sample_graphs)\n",
    "    pca_sample_feat.append(sample_feat)\n",
    "    pca_hc_models.append(hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-sweden",
   "metadata": {},
   "source": [
    "##### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num, item in enumerate(umap_data):\n",
    "    plot_dendrogram(item, \"UMAP\", layer_num, paths['Ward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_n_clusters = [7, 12, 12, 13]\n",
    "\n",
    "umap_sample_graphs = []\n",
    "umap_sample_feat = []\n",
    "umap_hc_models = []\n",
    "for layer_num, (item, n) in enumerate(zip(umap_data, umap_n_clusters)):\n",
    "    hc = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='ward')\n",
    "    pred_labels = hc.fit_predict(item)\n",
    "\n",
    "    d = (item, pred_labels)\n",
    "    plot_clusters(item, pred_labels, \"HC\", n, layer_num, paths['Ward'], \"UMAP\")\n",
    "    sample_graphs, sample_feat = plot_samples(hc, d, data[\"y\"], layer_num, n, \"HC\", \"UMAP reduced\", num_nodes_view, edges, num_expansions, paths['Ward'])\n",
    "\n",
    "    umap_sample_graphs.append(sample_graphs)\n",
    "    umap_sample_feat.append(sample_feat)\n",
    "    umap_hc_models.append(hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-facing",
   "metadata": {},
   "source": [
    "#### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer_num, sample in enumerate(raw_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"HC-Ward\", \"raw\", paths['Ward'])\n",
    "\n",
    "# for layer_num, sample in enumerate(tsne_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"HC-Ward\", \"t-SNE reduced\", paths['Ward'])\n",
    "\n",
    "# for layer_num, sample in enumerate(pca_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"HC-Ward\", \"PCA reduced\", paths['Ward'])\n",
    "\n",
    "# for layer_num, sample in enumerate(umap_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"HC-Ward\", \"UMAP reduced\", paths['Ward'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-sally",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "##### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_view = 5\n",
    "num_expansions = 3\n",
    "edges = data['edge_list'].numpy()\n",
    "\n",
    "esp = 0.1\n",
    "min_samples = 6\n",
    "\n",
    "raw_sample_graphs = []\n",
    "raw_dbscan_models = []\n",
    "for layer_num, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    dbscan_model = DBSCAN(eps=esp, min_samples=min_samples)\n",
    "    dbscan_model = dbscan_model.fit(activation)\n",
    "    pred_labels = dbscan_model.fit_predict(activation)\n",
    "\n",
    "    num_cluster = len(np.unique(pred_labels))\n",
    "    d = (activation, pred_labels)\n",
    "\n",
    "    plot_clusters(tsne_data[layer_num], pred_labels, \"DBSCAN\", num_cluster, layer_num, paths['DBSCAN'], \"Raw\", \"_TSNE\", \"(TSNE Reduced)\")\n",
    "    plot_clusters(pca_data[layer_num], pred_labels, \"DBSCAN\", num_cluster, layer_num, paths['DBSCAN'], \"Raw\", \"_PCA\", \"(PCA Reduced)\")\n",
    "    plot_clusters(pca_data[layer_num], pred_labels, \"DBSCAN\", num_cluster, layer_num, paths['DBSCAN'], \"Raw\", \"_UMAP\", \"(UMAP Reduced)\")\n",
    "    sample_graphs, sample_feat = plot_samples(dbscan_model, d, data[\"y\"], layer_num, num_cluster, \"DBSCAN\", \"raw\", num_nodes_view, edges, num_expansions, paths['DBSCAN'])\n",
    "\n",
    "    raw_sample_graphs.append(sample_graphs)\n",
    "    raw_dbscan_models.append((dbscan_model, num_cluster))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-kenya",
   "metadata": {},
   "source": [
    "##### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sample_graphs = []\n",
    "tsne_dbscan_models = []\n",
    "for layer_num, item in enumerate(tsne_data):\n",
    "    dbscan_model = DBSCAN(eps=esp, min_samples=min_samples)\n",
    "    dbscan_model = dbscan_model.fit(item)\n",
    "    pred_labels = dbscan_model.fit_predict(item)\n",
    "\n",
    "    num_cluster = len(np.unique(pred_labels))\n",
    "    d = (item, pred_labels)\n",
    "\n",
    "    plot_clusters(item, pred_labels, \"DBSCAN\", num_cluster, layer_num, paths['DBSCAN'], \"t-SNE\")\n",
    "    sample_graphs, sample_feat = plot_samples(dbscan_model, d, data[\"y\"], layer_num, num_cluster, \"DBSCAN\", \"t-SNE reduced\", num_nodes_view, edges, num_expansions, paths['DBSCAN'])\n",
    "\n",
    "    tsne_sample_graphs.append(sample_graphs)\n",
    "    tsne_dbscan_models.append((dbscan_model, num_cluster))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-mapping",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sample_graphs = []\n",
    "pca_dbscan_models = []\n",
    "for layer_num, item in enumerate(pca_data):\n",
    "    dbscan_model = DBSCAN(eps=esp, min_samples=min_samples)\n",
    "    dbscan_model = dbscan_model.fit(item)\n",
    "    pred_labels = dbscan_model.fit_predict(item)\n",
    "\n",
    "    num_cluster = len(np.unique(pred_labels))\n",
    "    d = (item, pred_labels)\n",
    "\n",
    "    plot_clusters(item, pred_labels, \"DBSCAN\", num_cluster, layer_num, paths['DBSCAN'], \"PCA\")\n",
    "    sample_graphs, sample_feat = plot_samples(dbscan_model, d, data[\"y\"], layer_num, num_cluster, \"DBSCAN\", \"PCA reduced\", num_nodes_view, edges, num_expansions, paths['DBSCAN'])\n",
    "\n",
    "    pca_sample_graphs.append(sample_graphs)\n",
    "    pca_dbscan_models.append((dbscan_model, num_cluster))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-williams",
   "metadata": {},
   "source": [
    "##### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_sample_graphs = []\n",
    "umap_dbscan_models = []\n",
    "for layer_num, item in enumerate(umap_data):\n",
    "    dbscan_model = DBSCAN(eps=esp, min_samples=min_samples)\n",
    "    dbscan_model = dbscan_model.fit(item)\n",
    "    pred_labels = dbscan_model.fit_predict(item)\n",
    "\n",
    "    num_cluster = len(np.unique(pred_labels))\n",
    "    d = (item, pred_labels)\n",
    "\n",
    "    plot_clusters(item, pred_labels, \"DBSCAN\", num_cluster, layer_num, paths['DBSCAN'], \"UMAP\")\n",
    "    sample_graphs, sample_feat = plot_samples(dbscan_model, d, data[\"y\"], layer_num, num_cluster, \"DBSCAN\", \"UMAP reduced\", num_nodes_view, edges, num_expansions, paths['DBSCAN'])\n",
    "\n",
    "    umap_sample_graphs.append(sample_graphs)\n",
    "    umap_dbscan_models.append((dbscan_model, num_cluster))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-slovakia",
   "metadata": {},
   "source": [
    "#### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer_num, sample in enumerate(raw_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"DBSCAN\", \"raw\", paths['DBSCAN'])\n",
    "\n",
    "# for layer_num, sample in enumerate(tsne_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"DBSCAN\", \"t-SNE reduced\", paths['DBSCAN'])\n",
    "\n",
    "# for layer_num, sample in enumerate(pca_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"DBSCAN\", \"PCA reduced\", paths['DBSCAN'])\n",
    "\n",
    "# for layer_num, sample in enumerate(umap_sample_graphs):\n",
    "#     plot_heuristics_table(ba_heuristics, sample, layer_num, \"DBSCAN\", \"UMAP reduced\", paths['DBSCAN'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-charity",
   "metadata": {},
   "source": [
    "# Activation to Concept to Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-grave",
   "metadata": {},
   "source": [
    "### Using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_str = \"decision_tree\"\n",
    "\n",
    "completeness_scores = []\n",
    "\n",
    "for i, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    activation_cls = ActivationClassifier(activation, raw_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"Raw\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths['KMeans'], i, k, \"raw\")\n",
    "    \n",
    "for i, item in enumerate(tsne_data):\n",
    "    activation_cls = ActivationClassifier(item, tsne_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "\n",
    "    d = [\"t-SNE reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths['KMeans'], i, k, \"t-SNE\")\n",
    "\n",
    "for i, item in enumerate(pca_data):\n",
    "    activation_cls = ActivationClassifier(item, pca_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "\n",
    "    d = [\"PCA reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths['KMeans'], i, k, \"PCA\")\n",
    "\n",
    "for i, item in enumerate(umap_data):\n",
    "    activation_cls = ActivationClassifier(item, umap_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "\n",
    "    d = [\"UMAP reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths['KMeans'], i, k, \"UMAP\")\n",
    "\n",
    "plot_completeness_table(\"k-Means\", \"Decision Tree\", completeness_scores, paths['KMeans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check training progress\n",
    "\n",
    "# x = data[\"x\"]\n",
    "# edges = data[\"edges\"]\n",
    "# y = data[\"y\"]\n",
    "# train_mask = data[\"train_mask\"]\n",
    "# test_mask = data[\"test_mask\"]\n",
    "# print(test(model, x, y, edges, test_mask))\n",
    "\n",
    "# activation = torch.squeeze(activation_list['conv2']).detach().numpy()\n",
    "# kmeans_model = KMeans(n_clusters=9, random_state=0)\n",
    "# kmeans_model = kmeans_model.fit(activation)\n",
    "\n",
    "# testing = ActivationClassifier(activation, kmeans_model, classifier_str, data[\"x\"], data[\"y\"], data[\"x\"], data[\"y\"], data[\"edges\"], i)\n",
    "# print(testing.accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_str = \"logistic_regression\"\n",
    "\n",
    "# completeness_scores = []\n",
    "\n",
    "# for i, key in enumerate(activation_list):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     activation_cls = ActivationClassifier(activation, raw_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"Raw\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths['KMeans'], i, k, \"raw\")\n",
    "    \n",
    "# for i, item in enumerate(tsne_data):\n",
    "#     activation_cls = ActivationClassifier(item, tsne_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"t-SNE reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths['KMeans'], i, k, \"t-SNE\")\n",
    "    \n",
    "# for i, item in enumerate(pca_data):\n",
    "#     activation_cls = ActivationClassifier(item, pca_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"PCA reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths['KMeans'], i, k, \"PCA\")\n",
    "    \n",
    "# for i, item in enumerate(umap_data):\n",
    "#     activation_cls = ActivationClassifier(item, umap_kmeans_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"UMAP reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths['KMeans'], i, k, \"UMAP\")\n",
    "    \n",
    "# plot_completeness_table(\"k-Means\", \"Logistic Regression\", completeness_scores, paths['KMeans'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-grammar",
   "metadata": {},
   "source": [
    "### Using Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-visiting",
   "metadata": {},
   "source": [
    "##### Using Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_str = \"decision_tree\"\n",
    "\n",
    "completeness_scores = []\n",
    "\n",
    "for i, (key, n) in enumerate(zip(activation_list, raw_n_clusters)):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    activation_cls = ActivationClassifier(activation, raw_ward_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"Raw\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"Ward\"], i, n, \"raw\")\n",
    "    \n",
    "for i, (item, n) in enumerate(zip(tsne_data, tsne_n_clusters)):\n",
    "    activation_cls = ActivationClassifier(item, tsne_hc_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"t-SNE reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"Ward\"], i, n, \"t-SNE\")\n",
    "    \n",
    "for i, (item, n) in enumerate(zip(pca_data, pca_n_clusters)):\n",
    "    activation_cls = ActivationClassifier(item, pca_hc_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"PCA reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"Ward\"], i, n, \"PCA\")\n",
    "    \n",
    "for i, (item, n) in enumerate(zip(umap_data, umap_n_clusters)):\n",
    "    activation_cls = ActivationClassifier(item, umap_hc_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"UMAP reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"Ward\"], i, n, \"UMAP\")\n",
    "\n",
    "plot_completeness_table(\"HC\", \"Decision Tree\", completeness_scores, paths[\"Ward\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_str = \"logistic_regression\"\n",
    "\n",
    "# completeness_scores = []\n",
    "\n",
    "# for i, key in enumerate(activation_list):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     activation_cls = ActivationClassifier(activation, raw_ward_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"Raw\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"Ward\"], i, n, \"raw\")\n",
    "    \n",
    "# for i, item in enumerate(tsne_data):\n",
    "#     activation_cls = ActivationClassifier(item, tsne_ward_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"t-SNE reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"Ward\"], i, n, \"t-SNE\")\n",
    "    \n",
    "# for i, item in enumerate(pca_data):\n",
    "#     activation_cls = ActivationClassifier(item, pca_ward_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"PCA reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"Ward\"], i, n, \"PCA\")\n",
    "    \n",
    "# for i, (item, n) in enumerate(zip(umap_data, umap_n_clusters)):\n",
    "#     activation_cls = ActivationClassifier(item, umap_ward_models[i], classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"UMAP reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"Ward\"], i, n, \"UMAP\")\n",
    "\n",
    "# plot_completeness_table(\"HC\", \"Logistic Regression\", completeness_scores, paths[\"Ward\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-therapy",
   "metadata": {},
   "source": [
    "##### Using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_str = \"decision_tree\"\n",
    "\n",
    "completeness_scores = []\n",
    "\n",
    "for i, key in enumerate(activation_list):\n",
    "    activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    \n",
    "    dbscan_model, n = raw_dbscan_models[i]\n",
    "    activation_cls = ActivationClassifier(activation, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"raw\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"DBSCAN\"], i, n, \"raw\")\n",
    "    \n",
    "for i, item in enumerate(tsne_data):\n",
    "    dbscan_model, n = tsne_dbscan_models[i]\n",
    "    activation_cls = ActivationClassifier(item, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"t-SNE reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"DBSCAN\"], i, n, \"t-SNE\")\n",
    "    \n",
    "for i, item in enumerate(pca_data):\n",
    "    dbscan_model, n = pca_dbscan_models[i]\n",
    "    activation_cls = ActivationClassifier(item, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"PCA reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"Ward\"], i, n, \"PCA\")\n",
    "    \n",
    "for i, item in enumerate(umap_data):\n",
    "    dbscan_model, n = umap_dbscan_models[i]\n",
    "    activation_cls = ActivationClassifier(item, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "    d = [\"UMAP reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "    completeness_scores.append(d)\n",
    "    activation_cls.plot(paths[\"DBSCAN\"], i, n, \"UMAP\")\n",
    "\n",
    "plot_completeness_table(\"DBSCAN\", \"Decision Tree\", completeness_scores, paths[\"DBSCAN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_str = \"logistic_regression\"\n",
    "\n",
    "# completeness_scores = []\n",
    "\n",
    "# for i, key in enumerate(activation_list):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     dbscan_model, n = raw_dbscan_models[i]\n",
    "#     activation_cls = ActivationClassifier(activation, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"raw\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"DBSCAN\"], i, n, \"raw\")\n",
    "    \n",
    "# for i, item in enumerate(tsne_data):\n",
    "#     dbscan_model, n = tsne_dbscan_models[i]\n",
    "#     activation_cls = ActivationClassifier(item, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"t-SNE reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"DBSCAN\"], i, n, \"t-SNE\")\n",
    "    \n",
    "# for i, item in enumerate(pca_data):\n",
    "#     dbscan_model, n = pca_dbscan_models[i]\n",
    "#     activation_cls = ActivationClassifier(item, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"PCA reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"DBSCAN\"], i, n, \"PCA\")\n",
    "    \n",
    "# for i, (item, n) in enumerate(zip(umap_data, umap_n_clusters)):\n",
    "#     dbscan_model, n = umap_dbscan_models[i]\n",
    "#     activation_cls = ActivationClassifier(item, dbscan_model, classifier_str, data[\"x\"], data[\"y\"], data[\"train_mask\"], data[\"test_mask\"], data[\"edge_list\"], i)\n",
    "    \n",
    "#     d = [\"UMAP reduced\", str(i), str(activation_cls.get_classifier_accuracy())]\n",
    "#     completeness_scores.append(d)\n",
    "#     activation_cls.plot(paths[\"DBSCAN\"], i, n, \"UMAP\")\n",
    "\n",
    "# plot_completeness_table(\"DBSCAN\", \"Logistic Regression\", completeness_scores, paths[\"DBSCAN\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-growing",
   "metadata": {},
   "source": [
    "# Graph Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_scores = []\n",
    "# view = 3\n",
    "# max_num_nodes = 15\n",
    "\n",
    "# for i, key in enumerate(activation_list):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     distances = get_node_distances(raw_kmeans_models[i], activation)\n",
    "\n",
    "#     for k_idx in range(k):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"KMeans\", \"Raw\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(raw_kmeans_models[i], activation, i, k, \"KMeans-Raw\", view, edges, num_expansions, paths['KMeans'])\n",
    "        \n",
    "# for i, item in enumerate(tsne_data):    \n",
    "#     distances = get_node_distances(tsne_kmeans_models[i], item)\n",
    "    \n",
    "#     for k_idx in range(k):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "\n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"KMeans\", \"TSNE\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(tsne_kmeans_models[i], item, i, k, \"KMeans-TSNE\", view, edges, num_expansions, paths['KMeans'])\n",
    "    \n",
    "# for i, item in enumerate(pca_data):\n",
    "#     distances = get_node_distances(pca_kmeans_models[i], item)\n",
    "    \n",
    "#     for k_idx in range(k):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"KMeans\", \"PCA\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(pca_kmeans_models[i], item, i, k, \"KMeans-PCA\", view, edges, num_expansions, paths['KMeans'])\n",
    "\n",
    "\n",
    "# for i, item in enumerate(umap_data):\n",
    "#     distances = get_node_distances(umap_kmeans_models[i], item)\n",
    "    \n",
    "#     for k_idx in range(k):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"KMeans\", \"UMAP\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(umap_kmeans_models[i], item, i, k, \"KMeans-UMAP\", view, edges, num_expansions, paths['KMeans'])\n",
    "    \n",
    "# plot_graph_similarity_table(\"Kmeans\", graph_scores, paths['KMeans'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_scores = []\n",
    "# view = 3\n",
    "# max_nodes = 15\n",
    "\n",
    "# for i, (key, n) in enumerate(zip(activation_list, raw_n_clusters)):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     pred_labels = raw_hc_models[i].fit_predict(activation)\n",
    "#     d_item = (activation, pred_labels)\n",
    "#     distances = get_node_distances(raw_hc_models[i], d_item)\n",
    "    \n",
    "#     for k_idx in range(n):        \n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "                \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"HC\", \"Raw\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(raw_hc_models[i], d_item, data[\"y\"], i, n, \"HC-Raw\", view, edges, num_expansions, paths['HC'])\n",
    "        \n",
    "        \n",
    "# for i, (item, n) in enumerate(zip(tsne_data, tsne_n_clusters)): \n",
    "#     pred_labels = tsne_hc_models[i].fit_predict(item)\n",
    "#     d_item = (item, pred_labels)\n",
    "#     distances = get_node_distances(tsne_hc_models[i], d_item)\n",
    "    \n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"HC\", \"TSNE\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(tsne_hc_models[i], d_item, data[\"y\"], i, n, \"HC-TSNE\", view, edges, num_expansions, paths['HC'])\n",
    "        \n",
    "\n",
    "        \n",
    "# for i, (item, n) in enumerate(zip(pca_data, pca_n_clusters)):\n",
    "#     pred_labels = pca_hc_models[i].fit_predict(item)\n",
    "#     d_item = (item, pred_labels)\n",
    "#     distances = get_node_distances(pca_hc_models[i], d_item)\n",
    "\n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"HC\", \"PCA\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(pca_hc_models[i], d_item, data[\"y\"], i, n, \"HC-PCA\", view, edges, num_expansions, paths['HC'])\n",
    "    \n",
    "\n",
    "# for i, (item, n) in enumerate(zip(umap_data, umap_n_clusters)):\n",
    "#     pred_labels = umap_hc_models[i].fit_predict(item)\n",
    "#     d_item = (item, pred_labels)\n",
    "#     distances = get_node_distances(umap_hc_models[i], d_item)\n",
    "\n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"HC\", \"UMAP\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(umap_hc_models[i], d_item, data[\"y\"], i, n, \"HC-UMAP\", view, edges, num_expansions, paths['HC'])\n",
    "        \n",
    "    \n",
    "# plot_graph_similarity_table(\"HC\", graph_scores, paths['HC'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_scores = []\n",
    "# view = 3\n",
    "# max_nodes = 15\n",
    "\n",
    "# for i, (key, n) in enumerate(zip(activation_list, raw_n_clusters)):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     pred_labels = raw_ward_models[i].fit_predict(activation)\n",
    "#     d_item = (activation, pred_labels)\n",
    "#     distances = get_node_distances(raw_ward_models[i], d_item)\n",
    "    \n",
    "#     for k_idx in range(n):        \n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "                \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"HC\", \"Raw\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(raw_ward_models[i], d_item, data[\"y\"], i, n, \"Ward-Raw\", view, edges, num_expansions, paths['Ward'])\n",
    "        \n",
    "        \n",
    "# for i, (item, n) in enumerate(zip(tsne_data, tsne_n_clusters)): \n",
    "#     pred_labels = tsne_ward_models[i].fit_predict(item)\n",
    "#     d_item = (item, pred_labels)\n",
    "#     distances = get_node_distances(tsne_ward_models[i], d_item)\n",
    "    \n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"Ward\", \"TSNE\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(tsne_ward_models[i], d_item, data[\"y\"], i, n, \"Ward-TSNE\", view, edges, num_expansions, paths['Ward'])\n",
    "        \n",
    "\n",
    "        \n",
    "# for i, (item, n) in enumerate(zip(pca_data, pca_n_clusters)):\n",
    "#     pred_labels = pca_ward_models[i].fit_predict(item)\n",
    "#     d_item = (item, pred_labels)\n",
    "#     distances = get_node_distances(pca_ward_models[i], d_item)\n",
    "\n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"Ward\", \"PCA\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(pca_ward_models[i], d_item, data[\"y\"], i, n, \"Ward-PCA\", view, edges, num_expansions, paths['Ward'])\n",
    "    \n",
    "\n",
    "# for i, (item, n) in enumerate(zip(umap_data, umap_n_clusters)):\n",
    "#     pred_labels = umap_ward_models[i].fit_predict(item)\n",
    "#     d_item = (item, pred_labels)\n",
    "#     distances = get_node_distances(umap_ward_models[i], d_item)\n",
    "\n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[k_idx])[::][:view]\n",
    "#         top_graphs, _, _ = get_top_subgraphs(top_indices, data[\"y\"], edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, max_nodes, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"Ward\", \"UMAP\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(umap_ward_models[i], d_item, data[\"y\"], i, n, \"Ward-UMAP\", view, edges, num_expansions, paths['Ward'])\n",
    "        \n",
    "    \n",
    "# plot_graph_similarity_table(\"Ward\", graph_scores, paths['Ward'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_scores = []\n",
    "# view = 3\n",
    "\n",
    "# for i, key in enumerate(activation_list):\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "    \n",
    "#     dbscan_model, n = raw_dbscan_models[i]\n",
    "#     distances = get_node_distances(dbscan_model, activation)\n",
    "\n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"DBSCAN\", \"Raw\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(dbscan_model, activation, i, k, \"DBSCAN-Raw\", view, edges, num_expansions, paths['DBSCAN'])\n",
    "        \n",
    "# for i, item in enumerate(tsne_data):  \n",
    "#     dbscan_model, n = tsne_dbscan_models[i]\n",
    "#     distances = get_node_distances(dbscan_model, item)\n",
    "    \n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "\n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"DBSCAN\", \"TSNE\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(dbscan_model, item, i, k, \"DBSCAN-TSNE\", view, edges, num_expansions, paths['DBSCAN'])\n",
    "    \n",
    "# for i, item in enumerate(pca_data):\n",
    "#     dbscan_model, n = pca_dbscan_models[i]\n",
    "#     distances = get_node_distances(dbscan_model, item)\n",
    "    \n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"DBSCAN\", \"PCA\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(dbscan_model, item, i, k, \"DBSCAN-PCA\", view, edges, num_expansions, paths['DBSCAN'])\n",
    "\n",
    "\n",
    "# for i, item in enumerate(umap_data):\n",
    "#     dbscan_model, n = umap_dbscan_models[i]\n",
    "#     distances = get_node_distances(dbscan_model, item)\n",
    "    \n",
    "#     for k_idx in range(n):\n",
    "#         top_indices = np.argsort(distances[:, k_idx])[::][:view]\n",
    "#         top_graphs, _ = get_top_subgraphs(top_indices, edges, num_expansions)\n",
    "        \n",
    "#         score = calc_graph_similarity(top_graphs, view)\n",
    "#         print(score)\n",
    "        \n",
    "#         d = [\"DBSCAN\", \"UMAP\", str(i), str(k_idx), str(score)]\n",
    "#         graph_scores.append(d)\n",
    "        \n",
    "#     plot_samples(dbscan_model, item, i, k, \"DBSCAN-UMAP\", view, edges, num_expansions, paths['DBSCAN'])\n",
    "    \n",
    "# plot_graph_similarity_table(\"DBSCAN\", graph_scores, paths['DBSCAN'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-tanzania",
   "metadata": {},
   "source": [
    "# GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_idx = 572\n",
    "\n",
    "# # convert to edge format\n",
    "# edges = edge_list.transpose(0, 1).t().contiguous()\n",
    "\n",
    "# explainer = GNNExplainer2(model, epochs=200, return_type='log_prob', log=True)\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx, node_data_x, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax, G = explainer.visualize_subgraph(node_idx, edges, edge_mask, y=node_data_y, threshold=0.8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-protest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
